# -*- coding: utf-8 -*-
"""Copy of Learning&first_try_Clip.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tWPGajZcc8PQnujAiZFnhQ8Vm1ZA6PSC
"""

# Commented out IPython magic to ensure Python compatibility.
# this mounts your Google Drive to the Colab VM.
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

# enter the foldername in your Drive where you have saved the unzipped
# assignment folder, e.g. 'UOW/AISecurity/assignment1/'
FOLDERNAME = 'UOW/998Image2Text/'
assert FOLDERNAME is not None, "[!] Enter the foldername."

# now that we've mounted your Drive, this ensures that
# the Python interpreter of the Colab VM can load
# python files from within it.
import sys
sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))
sys.path.append('/content/drive/My Drive/{}/codebase'.format(FOLDERNAME))

# %cd /content

# Commented out IPython magic to ensure Python compatibility.
# %cd "/content/drive/My Drive/UOW/998Image2Text"
!ls

import torch
import sys

device = torch.device('cuda' if torch.cuda.is_available else 'cpu')

print('PyTorch Version:', torch.__version__)
print('-' * 60)
if torch.cuda.is_available():
    print('CUDA Device Count:', torch.cuda.device_count())
    print('CUDA Device Name:')
    for i in range(torch.cuda.device_count()):
        print('\t', torch.cuda.get_device_name(i))
    print('CUDA Current Device Index:', torch.cuda.current_device())
    print('-' * 60)

print(f"Python version = {sys.version}")

# 安装依赖
!pip install ftfy regex tqdm
!pip install git+https://github.com/openai/CLIP.git

import torch
import clip
from PIL import Image
import matplotlib.pyplot as plt

# 加载模型
device = "cuda" if torch.cuda.is_available() else "cpu"
model, preprocess = clip.load("ViT-B/32", device=device)

# 上传两张图像（狗和飞机）——你可以在Colab点击左边“文件”图标上传
from google.colab import files
uploaded = files.upload()

# 读取图像并预处理
image_paths = list(uploaded.keys())
images = [preprocess(Image.open(p)).unsqueeze(0).to(device) for p in image_paths]
images = torch.cat(images)

# 文本输入
texts = clip.tokenize(["This is a dog.", "This is an airplane."]).to(device)

# 获取图像和文本的特征
with torch.no_grad():
    image_features = model.encode_image(images)
    text_features = model.encode_text(texts)

# 计算相似度（余弦相似度）
image_features /= image_features.norm(dim=-1, keepdim=True)
text_features /= text_features.norm(dim=-1, keepdim=True)
similarity = (100.0 * image_features @ text_features.T)

# 显示匹配结果
for i, path in enumerate(image_paths):
    print(f"Image: {path}")
    for j, text in enumerate(["This is a dog.", "This is an airplane."]):
        print(f"  '{text}': {similarity[i][j].item():.2f}")

# 下载 clip.py 和 model_loader.py
!wget https://raw.githubusercontent.com/rajpurkarlab/CXR-RePaiR/main/clip/clip.py
!wget https://raw.githubusercontent.com/rajpurkarlab/CXR-RePaiR/main/clip/model_loader.py
!wget https://raw.githubusercontent.com/rajpurkarlab/CXR-RePaiR/main/clip/model.py
!wget https://raw.githubusercontent.com/rajpurkarlab/CXR-RePaiR/main/clip/simple_tokenizer.py

!mv clip.py custom_clip.py
!mv model.py custom_model.py
!mv simple_tokenizer.py custom_tokenizer.py

!wget https://openaipublic.blob.core.windows.net/clip/bpe_simple_vocab_16e6.txt.gz -O /content/drive/MyDrive/UOW/998Image2Text/bpe_simple_vocab_16e6.txt.gz
vocab_path = "/content/drive/MyDrive/UOW/998Image2Text/bpe_simple_vocab_16e6.txt.gz"

!pip install ftfy

from custom_clip import load  # custom_clip.py 中的load()函数会自动构建并返回模型

model, _ = load(name="ViT-B/32", device="cpu", jit=False)

# 或者你可以传入参数后调用 model.encode_image() 等接口

# 加载训练好的 .pt 文件参数
import torch

model_path = "/content/drive/MyDrive/UOW/998Image2Text/clip-imp-pretrained_128_6_after_4.pt"
state_dict = torch.load(model_path, map_location="cpu")
model.load_state_dict(state_dict)
model.eval()  # 设置为推理模式

# ✅ 第一步：导入包
import torch
from PIL import Image
import torchvision.transforms as T
from custom_clip import load
from custom_tokenizer import SimpleTokenizer
import os

# ✅ 第二步：加载模型结构 & 权重
device = "cuda" if torch.cuda.is_available() else "cpu"
model, _ = load(name="ViT-B/32", device=device, jit=False)

# 加载你训练的模型参数
model_path = "/content/drive/MyDrive/UOW/998Image2Text/clip-imp-pretrained_128_6_after_4.pt"
state_dict = torch.load(model_path, map_location=device)
model.load_state_dict(state_dict)
model.eval()

# ✅ 第三步：定义图像预处理方式
preprocess = T.Compose([
    T.Resize(224, interpolation=Image.BICUBIC),
    T.CenterCrop(224),
    T.ToTensor(),
    T.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],
                std=[0.26862954, 0.26130258, 0.27577711]),
])

# ✅ 第四步：上传并处理图像
from google.colab import files
uploaded = files.upload()
image_paths = list(uploaded.keys())

images = [preprocess(Image.open(p).convert("RGB")).unsqueeze(0).to(device) for p in image_paths]
images = torch.cat(images)  # [N, 3, 224, 224]

# ✅ 第五步：处理文本
texts = ["This is a dog.", "This is an airplane."]
tokenizer = SimpleTokenizer()
tokenized = [tokenizer.encode(t) for t in texts]

# 把token列表变成张量，pad到同样长度
# max_len = max(len(t) for t in tokenized)
max_len = 77
input_text_tensor = torch.zeros(len(texts), max_len, dtype=torch.long).to(device)
for i, tokens in enumerate(tokenized):
    input_text_tensor[i, :len(tokens)] = torch.tensor(tokens)

# 打印，确认 pad 成功
print("input_text_tensor shape (should be [2, 77]):", input_text_tensor.shape)



# ✅ 第六步：提取特征
with torch.no_grad():
    image_features = model.encode_image(images)
    text_features = model.encode_text(input_text_tensor)

# ✅ 第七步：计算相似度
image_features /= image_features.norm(dim=-1, keepdim=True)
text_features /= text_features.norm(dim=-1, keepdim=True)
similarity = (100.0 * image_features @ text_features.T)

# ✅ 第八步：输出结果
for i, path in enumerate(image_paths):
    print(f"\nImage: {path}")
    for j, text in enumerate(texts):
        print(f"  '{text}': {similarity[i][j].item():.2f}")